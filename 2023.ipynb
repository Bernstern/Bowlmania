{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Input Data\n","\n","For this project, I pulled the last 10 years of bowl games and around 500 data points for each team every year. Note there were around 15k missing datapoints, in those cases we assume the team performed \"average\" and interpolate a value from the average of all other teams that year."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Requirement already satisfied: Unidecode in /home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages (1.3.7)\n"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import glob\n","import numpy as np\n","!pip install Unidecode\n","\n","from joblib import parallel_backend\n","\n","parallel_backend(\"loky\", n_jobs=30)\n","\n","# Reading all csv files starting with '20', adding a new column 'year' extracted from the filename, and concatenating them into a single DataFrame\n","historical_stats = pd.concat(\n","    [\n","        pd.read_csv(file).assign(\n","            year=int(file.split(\"-\")[0])\n","        )  # Reading each csv file and adding a 'year' column\n","        for file in glob.glob(\"20*.csv\")  # Finding all csv files starting with '20'\n","    ]\n",")\n","\n","# Reading a csv file named 'games.csv' into a DataFrame\n","historical_games = pd.read_csv(\"games.csv\")\n","games_trained_on = historical_games.shape[0] \n","\n","# Load in this years bowl games\n","bowl_games_2023 = pd.read_csv(\"bowl_games_2023.csv\")\n","num_eval_games = len(bowl_games_2023)\n","\n","# Get some fun stats\n","total_nans = historical_stats.isnull().sum().sum()\n","total_entries = historical_stats.size\n","\n","# SEED RANDOM\n","np.random.seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fixed_stats = historical_stats.copy()\n","\n","# Replace missing data points with the average\n","total_nans = fixed_stats.isnull().sum().sum()\n","total_entries = fixed_stats.size\n","fixed_stats.fillna(fixed_stats.mean(numeric_only=True), inplace=True)\n","\n","# Split win loss\n","fixed_stats[[\"Wins\", \"Losses\"]] = fixed_stats[\"Win-Loss\"].str.split(\n","    \"-\", expand=True\n",")\n","\n","# Bad Columns - These are data points without much reported data\n","fixed_stats.drop(['Time of Possession', 'Average Time of Possession per Game', 'Win-Loss'], axis=1, inplace=True, errors='ignore')\n","assert fixed_stats.isnull().sum().sum() == 0, \"Nans!\"\n","\n","# Remove duplicates\n","fixed_stats.drop_duplicates(subset=[\"Team\", \"year\"], keep=\"first\", inplace=True)\n","duplicates = fixed_stats[fixed_stats.duplicated([\"Team\", \"year\"])]\n","assert duplicates.shape[0] == 0, \"Duplicates found!\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from unidecode import unidecode\n","\n","def fix_team_names(df, cols=[\"0_team\", \"1_team\"]):\n","    def helper(team_name):\n","        team_name = team_name.rsplit(\"(\", 1)[0].strip()\n","        team_name = team_name.replace(\"St.\", \"State\")\n","        team_name = ''.join(filter(lambda x: (x.isalpha() or x == \" \") , team_name))\n","        team_name = team_name.lower()\n","        team_name = unidecode(team_name)\n","        return team_name\n","    \n","    for col in cols:\n","        df[col] = df[col].apply(helper)\n","\n","    return df\n","\n","problem = \"\"\n","def verify_team_stats_exist(frame, stats_df, team_cols=[\"0_team\", \"1_team\"]):\n","    not_found_teams = []\n","\n","    for _, row in frame.iterrows():\n","        year = row[\"year\"]\n","        teams =  [row[name] for name in team_cols]\n","        for team in teams:\n","            if stats_df[\n","                (stats_df[\"year\"] == year) & (stats_df[\"Team\"] == team)\n","            ].empty:\n","                not_found_teams.append((team, year, teams))\n","                problem = team\n","\n","    return not_found_teams\n","\n","def replace_and_save(df, old_string, new_string, filename):\n","    df.replace(old_string, new_string, inplace=True)\n","    df.to_csv(filename, index=False)\n","\n","def delete_team_rows(df, team_name,filename, cols=[\"0_team\", \"1_team\"]):\n","    for col in cols:\n","        df = df[df[col] != team_name]\n","    df.to_csv(filename, index=False)\n","    return df\n","\n","# Normalize all df team names\n","fixed_stats = fix_team_names(fixed_stats, cols=[\"Team\"])\n","bowl_games_2023 = fix_team_names(bowl_games_2023)\n","historical_games = fix_team_names(historical_games)\n","\n","# Check training data names\n","error = verify_team_stats_exist(historical_games, fixed_stats)\n","assert len(error) == 0, f\"Not all teams in historical game data found in historical stats {len(error)}\" + str(error[:5])\n","\n","# Check our evaluation data names\n","error = verify_team_stats_exist(bowl_games_2023, fixed_stats)\n","assert len(error) == 0, f\"Not all teams in bowl game history found in historical stats {len(error)}\" + str(error[:5])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_team_stats(games_df, stats_df, is_training_data=True):\n","    new_df = pd.DataFrame()\n","    new_df[\"year\"] = games_df[\"year\"]\n","    new_df[\"0_team\"] = games_df[\"0_team\"]\n","    new_df[\"1_team\"] = games_df[\"1_team\"]\n","\n","    if is_training_data: \n","        new_df[\"id_winner\"] = games_df[\"winner_id\"]\n","    else:\n","        new_df[\"id_winner\"] = None\n","\n","    # Merge historical stats for team 0\n","    new_df = pd.merge(\n","        new_df,\n","        stats_df.add_prefix(\"0_\"),\n","        how=\"inner\",\n","        left_on=[\"year\", \"0_team\"],\n","        right_on=[\"0_year\", \"0_Team\"],\n","    )\n","\n","    # Merge historical stats for team 1\n","    new_df = pd.merge(\n","        new_df,\n","        stats_df.add_prefix(\"1_\"),\n","        how=\"inner\",\n","        left_on=[\"year\", \"1_team\"],\n","        right_on=[\"1_year\", \"1_Team\"],\n","    )\n","\n","    # Remove the columns used to merge\n","    new_df = new_df.drop(columns=[\"0_team\", \"1_team\", \"0_year\", \"0_Team\", \"1_year\", \"1_Team\"])\n","\n","    return new_df\n","\n","# Attach stats to training and eval data\n","training_data = add_team_stats(historical_games, fixed_stats)\n","training_data_nums = training_data.size\n","evaluation_data = add_team_stats(bowl_games_2023, fixed_stats, is_training_data=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training\n","\n","Each model was trained on the whole data set, with 10-fold cross validation to determine its accuracy. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","models = {}\n","\n","from joblib import parallel_backend\n","\n","parallel_backend(\"loky\", n_jobs=30)\n","\n","\n","def train_model(pipeline, data=training_data):\n","    # Extract the features and target variable from the training data\n","    X_train = data.drop(\"id_winner\", axis=1)\n","    Y_train = data[\"id_winner\"]\n","\n","    # Train the model with 5-fold cross validation\n","    scores = cross_val_score(pipeline, X_train, Y_train, cv=5)\n","\n","    # Train the model on the entire dataset\n","    pipeline.fit(X_train, Y_train)\n","\n","    return pipeline, np.mean(scores)\n","\n","\n","def eval_model(name, model, data=training_data):\n","    trained_model, accuracy = train_model(model, data)\n","    models[name] = {\n","        \"model\": trained_model,\n","        \"accuracy\": accuracy,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Create a pipeline\n","pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"classifier\", GaussianNB())])\n","\n","eval_model(\"2022 Model\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Create a pipeline\n","pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"classifier\", GaussianNB())])\n","\n","eval_model(\"Naive Bayes\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Define the three classifiers to use in the ensemble\n","clf1 = GaussianNB()\n","clf2 = MLPClassifier(hidden_layer_sizes=(15,), max_iter=500)\n","clf3 = RandomForestClassifier(n_estimators=50)\n","\n","# Combine the classifiers in the ensemble model\n","ensemble_model = VotingClassifier(\n","    estimators=[(\"gnb\", clf1), (\"mlp\", clf2), (\"rf\", clf3)], voting=\"soft\"\n",")\n","\n","eval_model(\"Ensemble\", ensemble_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# Create pipelines with different scalers and logistic regression model\n","pipeline = Pipeline(\n","    [(\"scaler\", MinMaxScaler()), (\"logreg\", LogisticRegression(max_iter=1000))]\n",")\n","\n","# Fit the pipelines to the training data and print the accuracy for each one\n","eval_model(\"MM LogReg\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# Create pipelines with different scalers and logistic regression model\n","pipeline = Pipeline(\n","    [ ('scalar', MinMaxScaler()), ('pca', PCA(n_components=50)), (\"logreg\", LogisticRegression(max_iter=1000))]\n",")\n","\n","# Fit the pipelines to the training data and print the accuracy for each one\n","eval_model(\"PCA LogReg\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1211: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 30.\n","  warnings.warn(\n"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# Create pipelines with different scalers and logistic regression model\n","pipeline = Pipeline(\n","    [(\"scaler\", StandardScaler()), (\"logreg\", LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear'))]\n",")\n","\n","# Fit the pipelines to the training data and print the accuracy for each one\n","eval_model(\"MM LogReg*\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import VarianceThreshold\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","# Create a pipeline\n","pipeline = Pipeline(\n","    [\n","        (\"variance_threshold\", VarianceThreshold()),\n","        (\"scalar\", StandardScaler()),\n","        (\"logistic_regression\", LogisticRegression(max_iter=1000)),\n","    ]\n",")\n","\n","eval_model(\"VT LogReg\", pipeline)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.feature_selection import SelectFromModel\n","from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","\n","clf = Pipeline([\n","  ('feature_selection', SelectFromModel(LinearSVC())),\n","  ('classification', RandomForestClassifier())\n","])\n","\n","eval_model(\"FS + RFC\", clf)"]},{"cell_type":"markdown","metadata":{},"source":["# Model Confdience\n","\n","To the left is a plot of all the model's confidences relative to the ranked confidence. We can see that a Naive Bayes w/o any scaling becomes very confident very fast, much moreso than all other models. \n","\n","* Note the Y-Axis is grounded at 50% because all of these models cannot predict a winner with less than 50% confidence, bc then it would just pick the other team"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_accuracies = pd.DataFrame(\n","    [\n","        (name, model['accuracy'])\n","        for name, model in models.items()\n","    ],\n","    columns=[\"Model\", \"Training Accuracy\"],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions = dict()\n","\n","for model_name, model_details in models.items():\n","    model = model_details['model']\n","    predictions[model_name] = model.predict_proba(\n","        evaluation_data.drop(columns=[\"id_winner\"])\n","    )\n","\n","output = bowl_games_2023.copy()\n","\n","for model_name, predict in predictions.items():\n","    output[model_name + \"_predicted_winner\"] = predict.argmax(axis=1)\n","    output[model_name+ \"_confidence\"] = predict.max(axis=1)\n","    output[model_name + \"_rel_confidence\"] = output[model_name + \"_confidence\"].rank(ascending=True, method=\"min\")\n","    output[model_name + \"_predicted_winner_team\"] = output.apply(\n","            lambda row: row[\"0_team\"] if row[model_name + \"_predicted_winner\"] == 0 else row[\"1_team\"], axis=1\n","        )\n","\n","available_models_df = pd.DataFrame(list(models), columns=[\"Model\"])"]},{"cell_type":"markdown","metadata":{},"source":["# Model Inspection\n","\n","\n","Change what model you want to inspect its confidence when it comes to ranking"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json as _hex_json\n","view_model = _hex_pks.kernel_execution.input_cell.run_dropdown_dynamic(args=_hex_types.DropdownDynamicArgs.from_dict({**_hex_json.loads(\"{\\\"dataframe_column\\\":\\\"Model\\\",\\\"ui_selected_value\\\":\\\"MM LogReg*\\\"}\"), **{_hex_json.loads(\"\\\"options_variable\\\"\"):_hex_kernel.variable_or_none(\"available_models_df\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))\n","\n","import json as _hex_json\n","_hex_pks.kernel_execution.input_cell.filled_dynamic_value(args=_hex_types.FilledDynamicValueArgs.from_dict({**_hex_json.loads(\"{\\\"variable_name\\\":\\\"available_models_df\\\",\\\"dataframe_column\\\":\\\"Model\\\",\\\"max_size\\\":10000,\\\"max_size_in_bytes\\\":5242880}\"), **{_hex_json.loads(\"\\\"variable\\\"\"):_hex_kernel.variable_or_none(\"available_models_df\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["/tmp/ipykernel_12/3006726547.py:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  confidence_df[\"Projected Winner\"] = confidence_df.apply(\n"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["confidence_df = output[\n","    [\n","        \"0_team\",\n","        \"1_team\",\n","        \"{}_predicted_winner\".format(view_model),\n","        \"{}_rel_confidence\".format(view_model),\n","        \"{}_confidence\".format(view_model),\n","    ]\n","]\n","confidence_df.columns = [\n","    \"Team A\",\n","    \"Team B\",\n","    \"Projected Winner\",\n","    \"Ranked Confidence\",\n","    \"Confidence %\",\n","]\n","\n","confidence_df[\"Projected Winner\"] = confidence_df.apply(\n","            lambda row: row[\"Team A\"] if row[\"Projected Winner\"] == 0 else row[\"Team B\"], axis=1\n","        )"]},{"cell_type":"markdown","metadata":{},"source":["# Summary\n","\n","Last year a simple Naive Bayes Classifier was used to get lackluster performance, this year we are going with a Logistic Regression. Bring it"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json as _hex_json\n","select_which_model_to_save = _hex_pks.kernel_execution.input_cell.run_dropdown_dynamic(args=_hex_types.DropdownDynamicArgs.from_dict({**_hex_json.loads(\"{\\\"dataframe_column\\\":\\\"Model\\\",\\\"ui_selected_value\\\":\\\"PCA LogReg\\\"}\"), **{_hex_json.loads(\"\\\"options_variable\\\"\"):_hex_kernel.variable_or_none(\"available_models_df\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))\n","\n","import json as _hex_json\n","_hex_pks.kernel_execution.input_cell.filled_dynamic_value(args=_hex_types.FilledDynamicValueArgs.from_dict({**_hex_json.loads(\"{\\\"variable_name\\\":\\\"available_models_df\\\",\\\"dataframe_column\\\":\\\"Model\\\",\\\"max_size\\\":10000,\\\"max_size_in_bytes\\\":5242880}\"), **{_hex_json.loads(\"\\\"variable\\\"\"):_hex_kernel.variable_or_none(\"available_models_df\", scope_getter=lambda: globals())}}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_model = _hex_json.loads(\"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","selected_model = models[select_which_model_to_save]['model']\n","\n","# Save the model to disk\n","if save_model:\n","    filename = select_which_model_to_save + \".pkl\"\n","    pickle.dump(selected_model, open(filename, \"wb\"))\n","    print(f\"Saved {selected_model}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Biggest Takeaways\n","\n","1. More data => more better, last year we only trained on the last 10 years of bowl games, this year we expanded to the last 10 years of all FCS and FBS games (excluding games with a team not in one of those conferences)\n","2. Grid Searching parameters => instead of guess n checking hyper parameters, we actually did an exhaustive search \n","3. Using ChadGPT => Chad GPT was very helpful for coming up with different model types, namely the ensemble models\n","4. Using Hex => this site is actually insanely helpful for iterating"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["conf_df = pd.DataFrame()\n","\n","conf_df.insert(0, \"Ranked Confidence\", range(1, 43))\n","\n","for model_name, predict in predictions.items():\n","    temp = pd.DataFrame()\n","    temp[model_name + \" confidence\"] = predict.max(axis=1)\n","    temp[\"rel_confidence\"] = output[model_name + \"_confidence\"].rank(\n","        ascending=True, method=\"min\"\n","    )\n","    temp[\"rel_confidence\"] = temp[\"rel_confidence\"].astype(int)\n","    conf_df = pd.merge(conf_df, temp, left_on=\"Ranked Confidence\", right_on=\"rel_confidence\", how=\"left\")\n","    conf_df = conf_df.drop(columns=[\"rel_confidence\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["conf_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["|    | Team A             | Team B              | Projected Winner   |   Ranked Confidence |   Confidence % |\n","|---:|:-------------------|:--------------------|:-------------------|--------------------:|---------------:|\n","|  0 | ga southern        | ohio                | ohio               |                   9 |       0.591825 |\n","|  1 | howard             | florida am          | florida am         |                  30 |       0.794975 |\n","|  2 | jacksonville state | louisiana           | jacksonville state |                  36 |       0.875496 |\n","|  3 | miami oh           | app state           | miami oh           |                  38 |       0.891484 |\n","|  4 | new mexico         | fresno state        | fresno state       |                  35 |       0.868388 |\n","|  5 | ucla               | boise state         | ucla               |                  15 |       0.653845 |\n","|  6 | california         | texas tech          | texas tech         |                   8 |       0.59097  |\n","|  7 | western ky         | old dominion        | western ky         |                  24 |       0.719515 |\n","|  8 | utsa               | marshall            | utsa               |                  37 |       0.883676 |\n","|  9 | south fla          | syracuse            | south fla          |                  16 |       0.65527  |\n","| 10 | georgia tech       | ucf                 | georgia tech       |                  13 |       0.641005 |\n","| 11 | arkansas state     | niu                 | arkansas state     |                  26 |       0.762537 |\n","| 12 | troy               | duke                | troy               |                  40 |       0.93696  |\n","| 13 | georgia state      | utah state          | georgia state      |                  23 |       0.708443 |\n","| 14 | james madison      | air force           | james madison      |                  33 |       0.823555 |\n","| 15 | south alabama      | eastern mich        | south alabama      |                  31 |       0.808177 |\n","| 16 | utah               | northwestern        | utah               |                  25 |       0.724196 |\n","| 17 | coastal carolina   | san jose state      | coastal carolina   |                  22 |       0.701098 |\n","| 18 | bowling green      | minnesota           | bowling green      |                  18 |       0.662641 |\n","| 19 | texas state        | rice                | texas state        |                  14 |       0.648838 |\n","| 20 | kansas             | unlv                | kansas             |                   1 |       0.504707 |\n","| 21 | virginia tech      | tulane              | tulane             |                  39 |       0.893582 |\n","| 22 | north carolina     | west virginia       | north carolina     |                  28 |       0.777974 |\n","| 23 | louisville         | southern california | louisville         |                  41 |       0.942676 |\n","| 24 | texas am           | oklahoma state      | texas am           |                   7 |       0.559571 |\n","| 25 | smu                | boston college      | smu                |                  42 |       0.961554 |\n","| 26 | rutgers            | miami fl            | miami fl           |                   3 |       0.53502  |\n","| 27 | nc state           | kansas state        | nc state           |                  34 |       0.830115 |\n","| 28 | arizona            | oklahoma            | arizona            |                  10 |       0.604964 |\n","| 29 | clemson            | kentucky            | clemson            |                  27 |       0.77038  |\n","| 30 | oregon state       | notre dame          | notre dame         |                   2 |       0.507549 |\n","| 31 | memphis            | iowa state          | memphis            |                  32 |       0.822096 |\n","| 32 | missouri           | ohio state          | missouri           |                   6 |       0.55559  |\n","| 33 | ole miss           | penn state          | ole miss           |                  21 |       0.696878 |\n","| 34 | auburn             | maryland            | maryland           |                   4 |       0.537704 |\n","| 35 | georgia            | florida state       | georgia            |                   5 |       0.5452   |\n","| 36 | toledo             | wyoming             | toledo             |                  29 |       0.785772 |\n","| 37 | wisconsin          | lsu                 | lsu                |                  20 |       0.69092  |\n","| 38 | iowa               | tennessee           | iowa               |                  17 |       0.655844 |\n","| 39 | liberty            | oregon              | liberty            |                  19 |       0.664697 |\n","| 40 | alabama            | michigan            | alabama            |                  12 |       0.61886  |\n","| 41 | texas              | washington          | texas              |                  11 |       0.611484 |\n"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["print(confidence_df.to_markdown())"]}],"metadata":{"hex_info":{"author":"Bernie Conrad","exported_date":"Wed Dec 13 2023 02:28:40 GMT+0000 (Coordinated Universal Time)","project_id":"718bcbc7-d94f-407a-b4e0-40184904bbb7","version":"import"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":4}
