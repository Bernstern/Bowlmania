{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Kaggle datasets used\n",
    "# https://www.kaggle.com/datasets/mattop/college-football-bowl-games-1902-2022\n",
    "# https://www.kaggle.com/datasets/jeffgallini/college-football-team-stats-2019?select=cfb17.csv\n",
    "# https://www.kaggle.com/datasets/thedevastator/analyzing-college-football-2022-wins-losses-rank?select=games2022.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/cfb13.csv\n",
      "Loading data/cfb14.csv\n",
      "Loading data/cfb15.csv\n",
      "Loading data/cfb16.csv\n",
      "Loading data/cfb17.csv\n",
      "Loading data/cfb18.csv\n",
      "Loading data/cfb19.csv\n",
      "Loading data/cfb20.csv\n",
      "Loading data/cfb21.csv\n",
      "Concatenating stats\n"
     ]
    }
   ],
   "source": [
    "def load_stats() -> pd.DataFrame:\n",
    "    stats_list = []\n",
    "    # TODO: Include 2022\n",
    "    for yr in range(13, 22):\n",
    "        print(f\"Loading data/cfb{yr}.csv\")\n",
    "        df = pd.read_csv(f'data/cfb{yr}.csv')\n",
    "\n",
    "        # Make sure there are no unnamed columns\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        # Add year column\n",
    "        df['year'] = yr\n",
    "\n",
    "        stats_list.append(df)\n",
    "\n",
    "    print(\"Concatenating stats\")\n",
    "\n",
    "    stats_df = pd.concat(stats_list, sort=False)\n",
    "    # Drop any columns that are have any missing values\n",
    "    stats_df = stats_df.dropna(axis=1, how='any')\n",
    "\n",
    "    assert \"Team\" in stats_df.columns, \"Team column not found\"\n",
    "\n",
    "    # Sanitize the stats data\n",
    "    # Remove the division from the team name\n",
    "    stats_df['Team'] = stats_df['Team'].str.replace(r'\\([^\\(\\)]*\\)$', '')\n",
    "\n",
    "    # Strip out spaces and special characters\n",
    "    stats_df['Team'] = stats_df['Team'].str.replace(r'[^a-zA-Z0-9\\(\\)]', '')\n",
    "\n",
    "    # Remove the banned columns\n",
    "    BANNED_COLUMNS = [\n",
    "        \"Time.of.Possession\",\n",
    "        \"Average.Time.of.Possession.per.Game\",\n",
    "    ]\n",
    "    stats_df = stats_df.drop(columns=BANNED_COLUMNS)\n",
    "\n",
    "\n",
    "    # Remove any commmas from the numbers\n",
    "    stats_df.replace(',','', regex=True, inplace=True)\n",
    "\n",
    "    # Output it to a csv for funs sake\n",
    "    stats_df.to_csv('stats.csv', index=False)\n",
    "\n",
    "    # Multi-index\n",
    "    stats_multi_df = stats_df.set_index(['year', 'Team'])\n",
    "    stats_multi_df.to_csv('stats_m.csv')\n",
    "    return stats_multi_df\n",
    "\n",
    "stats = load_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    # Load up the all bowl games csv\n",
    "    bowl_games = pd.read_csv(\"data/all_bowl_games.csv\")\n",
    "\n",
    "    # see how many gamse were played in the last 9 years\n",
    "    bowl_games = bowl_games[bowl_games['year'] >= 2013]\n",
    "\n",
    "    # Write these back to a csv\n",
    "    bowl_games.to_csv('data/all_bowl_games.csv', index=False)\n",
    "\n",
    "    # Create a new dataframe with the columns we want\n",
    "    # training_data = pd.DataFrame(columns=['year', 'team1', 'team2', 'team1_win'] + [\"1_\" + col for col in team_data_cols] + [\"2_\" + col for col in team_data_cols])\n",
    "    columns = ['year', 'team0', 'team1', 'winning_team']\n",
    "    training_data = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for i, row in bowl_games.iterrows():\n",
    "        year = row[\"year\"] % 100\n",
    "        if np.random.rand() > .5:\n",
    "            new_row = [\n",
    "                year, \n",
    "                row['winner_tie'],\n",
    "                row['loser_tie'],\n",
    "                0,\n",
    "            ]   \n",
    "        else:\n",
    "            new_row = [\n",
    "                year, \n",
    "                row['loser_tie'],\n",
    "                row['winner_tie'],\n",
    "                1,\n",
    "            ]\n",
    "        # Add the add the row to the training data\n",
    "        training_data.loc[len(training_data)] = new_row # type: ignore\n",
    "    training_data.to_csv('training_data.csv', index=False)\n",
    "    return training_data\n",
    "\n",
    "training_data = load_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle name fixing\n",
    "def fix_names(name) -> str:\n",
    "    custom_mapping = {\n",
    "        \"Texas Christian\": \"TCU\",\n",
    "        \"Army\": \"Army West Point\",\n",
    "        \"Middle Tennessee State\": \"Middle Tenn\",\n",
    "        \"Florida International\": \"FIU\",\n",
    "        \"Alabama-Birmingham\": \"UAB\",\n",
    "        \"Central Florida\": \"UCF\",\n",
    "        \"South Florida\": \"South Fla\",\n",
    "        \"Miami\": \"Miami (FL)\",\n",
    "        \"Connecticut\": \"UConn\",\n",
    "        \"North Carolina State\": \"NCState\",\n",
    "        \"Appalachian State\": \"App State\",\n",
    "        \"Bringham Young\": \"BYU\",\n",
    "        \"Brigham Young\": \"BYU\",\n",
    "        \"Southern Methodist\": \"SMU\",\n",
    "        \"Florida Atlantic\": \"Fla Atlantic\",\n",
    "        \"Northern Illinois\": \"NIU\",\n",
    "        \"Southern Michigan\": \"Southern Mich\",\n",
    "        \"Central Michigan\": \"Central Mich\",\n",
    "        \"Western Michigan\": \"Western Mich\",\n",
    "        \"Eastern Michigan\": \"Eastern Mich\",\n",
    "        \"Georgia Southern\": \"Ga Southern\",\n",
    "        \"Bowling Green State\": \"Bowling Green\",\n",
    "        \"Western Kentucky\": \"Western Ky\",\n",
    "        \"Louisiana State\": \"Louisiana\",\n",
    "        \"Colorado State\": \"Colorado\",\n",
    "        \"Utah State\": \"Utah\",\n",
    "        \"Texas-San Antonio\": \"UTSA\",\n",
    "        \"Texas A&M\": \"Texas AM\",\n",
    "        \"Southern Ole Miss\": \"Ole Miss\",\n",
    "        \"Nevada-Las Vegas\": \"UNLV\",\n",
    "        \"Texas-El Paso\": \"UTEP\",\n",
    "        \"New Mexico\": \"New Mexico State\",\n",
    "    }\n",
    "\n",
    "    # Check if the name is in the custom mapping\n",
    "    name = custom_mapping.get(name, name)\n",
    "\n",
    "    # Remove special characters\n",
    "    name = name.replace(r'[^a-zA-Z0-9\\(\\)]', '')\n",
    "\n",
    "    # Convert State to St\n",
    "    name = name.replace('State', 'St')\n",
    "\n",
    "    # Remove spaces\n",
    "    name = name.replace(' ', '')\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing training data!\n"
     ]
    }
   ],
   "source": [
    "def teams_to_stats(stats, year, team0, team1):\n",
    "\n",
    "    team0 = fix_names(team0)\n",
    "    team1 = fix_names(team1)\n",
    "\n",
    "    # Check that the teams are in the stats\n",
    "    if team0 not in stats.loc[year].index:\n",
    "        # Find the closest name to the team\n",
    "        closest = process.extract(team0, stats.loc[year].index)[0]\n",
    "\n",
    "        raise Exception(f'{team0} not in stats for year {year} - did you mean {closest}?')\n",
    "\n",
    "    if team1 not in stats.loc[year].index:\n",
    "        closest = process.extract(team1, stats.loc[year].index)[0]\n",
    "\n",
    "        raise Exception(f'{team1} not in stats for year {year} - did you mean {closest}?')\n",
    "\n",
    "    # df = pd.concat([stats.loc[year,team0], stats.loc[year,team1]], axis=1)\n",
    "    # print(df.loc[:,(16,'Buffalo (MAC)')])\n",
    "    df = stats.loc[(year,[team0, team1]), :]\n",
    "    big_vector = list(df.iloc[0]) + list(df.iloc[1])\n",
    "    return big_vector\n",
    "\n",
    "converted_training_data = []\n",
    "\n",
    "# Create a df of new training data using the team_to_stats function with the input from the training data df\n",
    "for i, row in training_data.iterrows():\n",
    "    converted_training_data.append(teams_to_stats(stats, row['year'], row['team0'], row['team1']) + [row['winning_team']])\n",
    "\n",
    "print(\"Done processing training data!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1,045'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m converted_training_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(converted_training_data)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Convert every cell into a float\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m converted_training_data \u001b[39m=\u001b[39m converted_training_data\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m X \u001b[39m=\u001b[39m converted_training_data\u001b[39m.\u001b[39miloc[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m y \u001b[39m=\u001b[39m converted_training_data\u001b[39m.\u001b[39miloc[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   6241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:450\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m    528\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    232\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    169\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1,045'"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with the columns we want, the last column is the winning team\n",
    "converted_training_data = pd.DataFrame(converted_training_data)\n",
    "\n",
    "# Convert every cell into a float\n",
    "converted_training_data = converted_training_data.astype(float)\n",
    "\n",
    "X = converted_training_data.iloc[:, :-1]\n",
    "y = converted_training_data.iloc[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is time to learn\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1,119'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the transformer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m transformer \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 3\u001b[0m transformer\u001b[39m.\u001b[39;49mfit(X_train)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    862\u001b[0m     X,\n\u001b[0;32m    863\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    864\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    865\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    866\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 535\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    536\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    537\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:877\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    875\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    876\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    878\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    879\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    881\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1,119'"
     ]
    }
   ],
   "source": [
    "# Train the transformer\n",
    "transformer = StandardScaler()\n",
    "transformer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
